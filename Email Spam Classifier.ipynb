{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8928f35",
   "metadata": {},
   "source": [
    "#### Objective: Build a spam classifier using Bayes' Theorem and a data set of emails labeled as spam or non-spam.\n",
    "\n",
    "#### Objetivo: Construir un clasificador de correo no deseado (spam) utilizando el Teorema de Bayes y un conjunto de datos de correos electrónicos etiquetados como spam o no spam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178f959b",
   "metadata": {},
   "source": [
    "#### Import lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d490ac3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac45e61c",
   "metadata": {},
   "source": [
    "##### Load file csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81d8c647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_or_ham = pd.read_csv(\"spam.csv\", encoding='latin-1')[[\"v1\", \"v2\"]]\n",
    "spam_or_ham.columns = [\"label\", \"text\"]\n",
    "spam_or_ham.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97469c55",
   "metadata": {},
   "source": [
    "##### Filter within spam and ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07addb89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_or_ham[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc7af3f",
   "metadata": {},
   "source": [
    "#### Optimization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c651ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punctuation = set(string.punctuation)\n",
    "def tokenize(sentence):\n",
    "    tokens = []\n",
    "    for token in sentence.split():\n",
    "        new_token = []\n",
    "        for character in token:\n",
    "            if character not in punctuation:\n",
    "                new_token.append(character.lower())\n",
    "        if new_token:\n",
    "            tokens.append(\"\".join(new_token))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da15179",
   "metadata": {},
   "source": [
    "##### Use tokenization algoritm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "211bf0bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [go, until, jurong, point, crazy, available, o...\n",
       "1                       [ok, lar, joking, wif, u, oni]\n",
       "2    [free, entry, in, 2, a, wkly, comp, to, win, f...\n",
       "3    [u, dun, say, so, early, hor, u, c, already, t...\n",
       "4    [nah, i, dont, think, he, goes, to, usf, he, l...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_or_ham.head()[\"text\"].apply(tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ea6d66",
   "metadata": {},
   "source": [
    "#### We are going to use the scikit-learn library to do the heavy lifting of the learning process and testing. We are saying what function you have to use for tokenization and that it must be binary.\n",
    "### we separate the data between training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee5d5e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "demo_vectorizer = CountVectorizer(\n",
    "    tokenizer = tokenize,\n",
    "    binary = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df4f4eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 4179, testing examples 1393\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_text, test_text, train_labels, test_labels = train_test_split(spam_or_ham[\"text\"], spam_or_ham[\"label\"], stratify=spam_or_ham[\"label\"])\n",
    "print(f\"Training examples: {len(train_text)}, testing examples {len(test_text)}\")\n",
    "\n",
    "# stratify (to ensure that the data split between the training set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b5d2a8",
   "metadata": {},
   "source": [
    "#### We were left with 4179 examples for training and 1393 examples for testing. We create a new vectorizer, from scratch, in which we are only going to use the training data\n",
    "### We will NOT use test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ca5e632",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_vectorizer = CountVectorizer(tokenizer = tokenize, binary=True)\n",
    "train_X = real_vectorizer.fit_transform(train_text)\n",
    "test_X = real_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1ab676",
   "metadata": {},
   "source": [
    "#### We create the new classifier and use the fit() method to process the data, which prepares the classifier for later use. Again, we use the training data to prepare it, not the test data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5798e45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "classifier = LinearSVC()\n",
    "classifier.fit(train_X, train_labels)\n",
    "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
    "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
    "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
    "          verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98e0db1",
   "metadata": {},
   "source": [
    "#### The classifier is ready to work\n",
    "#### We use predict() to measure the accuracy of the model with data test . \n",
    "#### The scikit-learn function called accuracy_score() helps us calculate the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b3a8313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.2053%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predicciones = classifier.predict(test_X)\n",
    "accuracy = accuracy_score(test_labels, predicciones)\n",
    "print(f\"Accuracy: {accuracy:.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa856843",
   "metadata": {},
   "outputs": [],
   "source": [
    "frases = [\n",
    "  'Are you looking to redesign your website with new modern look and feel?',\n",
    "  'Please send me a confirmation of complete and permanent erasure of the personal data',\n",
    "  'You have been selected to win a FREE suscription to our service',\n",
    "  'We’re contacting you because the webhook endpoint associated with your account in test mode has been failing',\n",
    "  'Confirma tu cuenta de Facebook en el siguiente link',\n",
    "  'You have been selected to participate in a free service'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab01b8a",
   "metadata": {},
   "source": [
    "#### We use our transformation and vectorization algorithm, to finally receive the classification predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5efc592",
   "metadata": {},
   "outputs": [],
   "source": [
    "frases_X = real_vectorizer.transform(frases)\n",
    "predicciones = classifier.predict(frases_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02869091",
   "metadata": {},
   "source": [
    "#### show results and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "906d8d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam  - Are you looking to redesign your website with new modern look and feel?\n",
      "ham   - Please send me a confirmation of complete and permanent erasure of the personal data\n",
      "spam  - You have been selected to win a FREE suscription to our service\n",
      "ham   - We’re contacting you because the webhook endpoint associated with your account in test mode has been failing\n",
      "ham   - Confirma tu cuenta de Facebook en el siguiente link\n",
      "spam  - You have been selected to participate in a free service\n"
     ]
    }
   ],
   "source": [
    "for text, label in zip(frases, predicciones):\n",
    "  print(f\"{label:5} - {text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
